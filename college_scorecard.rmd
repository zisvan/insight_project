---
title: "College Scorecard Notes"
output: html_notebook
---
### Data
Data from Department of Energy's [College Scorecard](https://collegescorecard.ed.gov/data/).  It's called raw data but it is not the rawest - it's been aggregated to the school level, but contains a lot of information.  Many variables' means, medians, and five quintiles are reported even though individual student data isn't available.

Not each year has data for each variable, they changed what they collected over the years.

NA's are called NULL's.

Start with one file for now, the latest year that has the information I want is 12-13. 
To do: Join all the files - each school has a unique ID. Look at time dependence, or select schools that have been in top N for m years.

This is the data selection. 
```{r warning=FALSE}
# get one file for now from 11-12
# data <- read.csv("../college_data/MERGED2011_12_PP.csv", na.strings = "NULL")

# variables to selet
colNames <- c("OPEID","OPEID6","MD_EARN_WNE_P10", "MD_EARN_WNE_P6", "MN_EARN_WNE_P6", "MN_EARN_WNE_P10",
               "MD_FAMINC", "DEP_INC_AVG", "IND_INC_AVG",
              "MN_EARN_WNE_INDEP0_P10", "MN_EARN_WNE_INDEP1_P10")

colNums <- match(colNames, names(data))

four_year <- data %>%
  #na.omit() %>%
  filter(ICLEVEL == 1) %>%
  select(colNums) %>%
  filter(complete.cases(.)) %>%
  distinct()


# convert factors to numeric
four_year[colNames] <- sapply(four_year[colNames], as.character)
four_year[colNames] <- sapply(four_year[colNames], as.numeric)

#remove newly introduced NAs by numericizing
four_year <- na.omit(four_year)
```

### Select good earners
Want a metric to the effect of 'this school produces good earners.' Picking one number, say above 50k, would not account for correlation between family income and college grad income. Make a very rudimentary first order correction for this - plots below show mean income 10 years after enrollment as a function of family income at time of enrollment. 

```{r}
# linear fit
p_lin <- ggplot(four_year, aes(x = DEP_INC_AVG, y = MN_EARN_WNE_INDEP0_P10)) + 
  geom_point(alpha = 0.2, color = "blue") + 
  geom_smooth(method = "lm", formula = y ~ x, color = "red") + 
  xlab("Mean family income (2015 $s)") +
  ylab("Mean earning 10 years after enrollment (2015 $s)")


p_lin
```

Low income students all fall under the line.  Not surprising, and not *that* telling - their adult earnings likely surpass their parents' later in life. The plan is to cut above this line - don't want to lose these points.  Fit default smooth?

```{r}
p2 <- ggplot(four_year, aes(x = DEP_INC_AVG, y = MN_EARN_WNE_INDEP0_P10)) + 
  geom_point(alpha = 0.2, color = "blue") + 
  geom_smooth(color = "red") + 
  xlab("Mean family income (2015 $s)") +
  ylab("Mean earning 10 years after enrollment (2015 $s)")
p2
```

That's basically three lines.  (Well, ok.  Not really.)  Let's see anyway.  Lines may be easier to cut on.
```{r}
# Add facet for parents' income bracket
four_year <- mutate(four_year, fam_bracket = ifelse(DEP_INC_AVG < 30000, "Low",
                        ifelse(DEP_INC_AVG > 75000, "High", "Mid")))

# Make facets into factors - orders better in ggplot
four_year$fam_bracket_f <- factor(four_year$fam_bracket, levels = c("Low", "Mid", "High"))

p_bracket <- ggplot(four_year, aes(x = DEP_INC_AVG, y = MN_EARN_WNE_INDEP0_P10)) + 
  geom_point(alpha = 0.2, color = "blue") + 
  geom_smooth(method = "lm", formula = y ~ x, color = "red") + 
  xlab("Mean family income (2015 $s)") +
  ylab("Mean earning 10 years after enrollment (2015 $s)") +
  facet_grid(. ~ fam_bracket_f, scale = "free")

p_bracket
```

This is somewhat meaningful, but the dots are schools, not students. The goal was to come up with a non-horizontal line above which one might call 'alumni make good money' without biasing against low income. A bit of an arbitrary cut - will need to investigate further what I am leaving out. 

Turns out it's difficult to extract the fit results from ggplot. (Some hacky ways on stack overflow, but do the fits standalone instead).

Here are the three lines:
```{r}
fitLo <- lm(formula = MN_EARN_WNE_INDEP0_P10 ~ DEP_INC_AVG, 
          data = four_year, fam_bracket_f == "Low")
summary(fitLo)
```

```{r}
fitMd <- lm(formula = MN_EARN_WNE_INDEP0_P10 ~ DEP_INC_AVG, 
            data = four_year, fam_bracket_f == "Mid")
summary(fitMd)
```

```{r}
fitHi <- lm(formula = MN_EARN_WNE_INDEP0_P10 ~ DEP_INC_AVG, 
            data = four_year, fam_bracket_f == "High")
summary(fitHi)

```

The goodness of these is not a big deal - they are somewhat arbitrary lines, and not actually used as a fit result. Let's make sure that these are the same as what ggplot gave. 

```{r}
# get the slope and intercept into its own df, kind of hacky to get legend right

fits <- data.frame(m = c(summary(fitLo)$coeff[2], 
                           summary(fitMd)$coeff[2],
                           summary(fitHi)$coeff[2]),
                  b = c(summary(fitLo)$coeff[1],
                               summary(fitMd)$coeff[1], 
                               summary(fitHi)$coeff[1]),
                  br = c("Low", "Mid", "High"))

p_bracket_manual <- ggplot(four_year, aes(x = DEP_INC_AVG, 
                                          y = MN_EARN_WNE_INDEP0_P10)) + 
  geom_point(alpha = 0.2) + 
  geom_abline(aes(slope = m, intercept = b, color = br),size = 1, data = fits) +
  xlab("Mean family income (2015 $s)") +
  ylab("Mean earning 10 years after enrollment (2015 $s)") +
  facet_grid(. ~ fam_bracket_f, scale = "free")
p_bracket_manual

```

The lines are the same: Good. This plot is ugly at the moment, but plotting all the lines on all the brackets was informative. 

* Low income students more likely than mid or high to do better than their parents, which makes their standard of doing better than their parents too high a bar for mid to hihg income students.
* Mid and high income students bracket widths get cut righ in the middle, causing ill fits.  One option is to come up with a better binning, but this is the binning that the dataset aggregated many ready-to-use variables.  Long term - do my own binning and use numbers, not means and percentiles from data. 
* Bottom line: Stick with original plan to select above each bracket's line.


```{r}
# Simple function to determine if graduates of school 
# are considered good earners
good_earner <- function(y, x, bracket){
  slope = fits[fits$br == bracket,"m"]
  #print(slope)
  intercept = fits[fits$br == bracket,"b"]
  #print(intercept)
  
  if (y >= slope*x + intercept){
    #print("True")
    return(TRUE)
  } 
  else{
    #print("False")
    return(FALSE)
    }
}

# test <- good_earner(2000, 20000, "Low")

# add earning column
four_year_earn <- four_year %>%
  rowwise() %>%
  mutate(earnings = ifelse(good_earner(MN_EARN_WNE_INDEP0_P10,
                                            DEP_INC_AVG,
                                            fam_bracket),"Above", "Below")) 

# Get IDs of selection so far to bind to full dataset
earn_id <- four_year_earn %>%
  select(OPEID, OPEID6, fam_bracket_f, earnings)
                    
# Bind this with full dataset to keep only the selected schools but all variables
keep_schools <- merge(data, earn_id, by=c("OPEID","OPEID6"))
keep_schools <- subset(keep_schools, !duplicated(keep_schools[,"OPEID"]))
```

Let's check that the remaining data is above the cut line.
```{r}
p_good <- ggplot(data = subset(four_year_earn, earnings=="Above"), aes(x = DEP_INC_AVG, 
                                          y = MN_EARN_WNE_INDEP0_P10)) + 
  geom_point(alpha = 0.2) + 
  geom_abline(aes(slope = m, intercept = b, color = br),size = 1, data = fits) +
  xlab("Mean family income (2015 $s)") +
  ylab("Mean earning 10 years after enrollment (2015 $s)") +
  facet_grid(. ~ fam_bracket_f, scale = "free")
p_good

```

Seems to have worked! The selection is greater than or equal to - that's why there are dots in the boundary.

One thing to note is that this is not a robust definition of producing good earner by any means.  Next iteration of analysis needs to include a (much) better selection method than this arbitrary line.  But let's keep going with building the skeleton.

### Predict good schools?
Can I predict schools that will produce good earners?  R has a built in knn function - start there.

```{r}
gradVars <- c("C150_4", #completion rate
              "COSTT4_A", #average cost of attendance academic year
              "TUITFTE", #Net tuition revenue per full-time equivalent student
              "INEXPFTE", #Instructional expenditures per full-time equivalent student
              "ADM_RATE", #Admission rate overall
              "SAT_AVG", #Average SAT equivalent score of students admitted
              "UGDS", #Enrollment of all undergraduate students
              "AVGFACSAL", #Average faculty salary
              "PFTFAC", #Ratio of full-time faculty
              "DEBT_MDN", #Median debt
              "PAR_ED_PCT_1STGEN", #percentage of first-gen students
              "earnings" #Earnings Above or Below
)
```

**TO DO:** This list of parameters have been chosen extremly subjectively.  I selected them by scrolling down the data dictionary.  I deliberately avoided potentially good predictors such as race because I want to find school parameters, not social ones. Future iterations need to more quantitatively onbectively make this selection, maybe clean up the data and throw a principle components analysis at it? I used the same set that I used to predict graduation rate.  

```{r warning=FALSE}
colGradVars <- match(gradVars, names(keep_schools))

knnSample <- select(keep_schools, colGradVars)

#convert all but earnings to numeric (they ae already chr)
numeric.num <- match(gradVars[1:length(gradVars)-1], names(knnSample))
knnSample[numeric.num] <- sapply(knnSample[numeric.num], as.numeric)

#remove NAs from converting
knnSample <- knnSample[complete.cases(knnSample),]


#normalize columns to same scale
normalize <- function(x){
  if(!is.numeric(x)){
    return(x)
  }else{
    norm_x <- (x-min(x))/(max(x) - min(x))
    return(norm_x)    
  }
}

knnNorm <- as.data.frame(sapply(knnSample, normalize))


#training and test samples
# Sample into training and test sets
set.seed(123)
trn <- sample(seq_len(nrow(knnNorm)), size = floor(.75 * nrow(knnNorm)))

train <- knnNorm[trn, ]
test <- knnNorm[-trn, ]

#run knn
kn <- floor(sqrt(nrow(train)))
pred.knn <- knn(train = train[, 1:11], test = test[, 1:11], cl = train[, 12],
                   k = kn)



table(test[,12],pred.knn)
prop.table(table(test[,12],pred.knn))
```
Not very good.  Biases heavily towards Below.  Manually check a few other k's.

```{r}
pred.knn_5 <- knn(train = train[, 1:11], test = test[, 1:11], cl = train[, 12],
                k = 5)
pred.knn_11 <- knn(train = train[, 1:11], test = test[, 1:11], cl = train[, 12],
                k = 11)
pred.knn_19 <- knn(train = train[, 1:11], test = test[, 1:11], cl = train[, 12],
                k = 19)
table(test[,12],pred.knn_5)
table(test[,12],pred.knn_11)
table(test[,12],pred.knn_19)
```

They perform about equally well.  To answer the hard question - out of these (then, out of more variables) which ones are predictive. 

### Improve kNN

This biserial correlation function seems potentially useful.  Here's some play with it.  

**TO DO:** Determine which variables to use in the knn - I don't yet understand this function well enough.  
```{r}
#calculate correlation between each variable and earnings
bicorr <- c()
for(i in 1:11){
  bicorr[i] <- biserial.cor(knnNorm[,i], knnNorm[,12])
}

bi_df <- as.data.frame(bicorr)
rownames(bi_df) <- names(knnNorm[,1:11])
bi_df <- cbind(Variable = rownames(bi_df), bi_df)
rownames(bi_df) <- NULL

bi_df <- arrange(bi_df, desc(abs(bicorr)))
bi_df
```

With this set of variables, try to find combinations that work better. 

<!-- Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*. -->

<!-- When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file). -->
